{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"font-size: 45px; font-weight:bold; margin:20px; margin-bottom:100px; text-align: justify; text-shadow: 1px 1px 1px #919191,\n",
    "        1px 2px 1px #919191,\n",
    "        1px 3px 1px #919191,\n",
    "        1px 4px 1px #919191,\n",
    "        1px 5px 1px #919191,\n",
    "        1px 6px 1px #919191,\n",
    "        1px 7px 1px #919191,\n",
    "        1px 8px 1px #919191,\n",
    "        1px 9px 1px #919191,\n",
    "        1px 10px 1px #919191,\n",
    "    1px 18px 6px rgba(16,16,16,0.4),\n",
    "    1px 22px 10px rgba(16,16,16,0.2),\n",
    "    1px 25px 35px rgba(16,16,16,0.2),\n",
    "    1px 30px 60px rgba(16,16,16,0.4)\"> GENERATIVE ADVERSARIAL NETWORKS </div>\n",
    "    \n",
    "<div style=\"font-style: italic; font-weight: bold; font-size:35px; text-align:center; font-family: Garamond\">by Rubén Cañadas Rodríguez</div>\n",
    "\n",
    "<div style=\"font-size: 30px; margin: 20px; margin-bottom: 40px; margin-left: 0px; line-height: 40pt\">\n",
    "\n",
    "<div style=\"font-size: 30px; font-family: Garamond; font-weight: bold; margin: 30px; margin-left: 0px; margin-bottom: 10px; \">Contents</div>\n",
    "<ol>\n",
    "<li>Introduction</li>\n",
    "<li>Generative Adversarial Networks</li>\n",
    "<li>Applications</li> \n",
    "<li>Conclusions</li> \n",
    "<li>Coding</li> \n",
    "</ol>\n",
    "</div></div>\n",
    "\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Introduction </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "In this tutorial we will be implementing a generative method called Generative Adversarial Networks, also known as GANs. This is the base for other similar deep learning architectures such as Pix2Pix, StyleGan or Conditional Gans that we will see later in other IA tutorials. This architecture can be similar to the autoencoder ones. However, in this case not only we have a generator but also a discriminator. Summing up (we will get more in detail in the following sections of the tutorial) the generator samples noise from a random distribution and a generator is trained with real data in order to know if a certain image is real or fake. Then, the two architectures are trained in order to fool the other. The generator gets better and better so as the discriminator cannot differ from a real image to a generated one, and the discriminator gets better and betters at noticing whether and image is real or not. \n",
    "In this case the chosen deep learning framework will be Pytorch.</div>\n",
    "    \n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Generative Adversarial Networks</div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "As mentioned in the introduction GANs are built using two different neural networks, the generator which will generate images, and the discriminator which will be a real/fake classifier. In this case we are dealing with CIFAR10 dataset and a human face dataset. Therefore, the basic layers that will build the networks will be convolutional layers since they work really well for high-dimensional tensors such as images (images can be seen as three-dimensional tensors, having as dimensions: height and width which are defined by the pixels, and also three channels (RGB) if we are working with colored-images. For black-white images such as the MNIST dataset there is only one channel). The discriminator will be a conventional Convolutional Neural Network. Nevertheless, the generator has to perform the inverse transformation. Thus, we need a deconvolution instead of a convolution operation. Hence, the generator is a Deconvolutional Neural Network (DNN). As you probably have observed, actually the generator is a decoder (as we saw in the Variational Autoencoder tutorial) with no encoder. In this case we are not trying to infer the training distribution function using a encoder, we will simply sample from a Gaussian distribution (which is considered as random noise). And from that random vector sampled from the Gaussian distribution the generator (decoder) will construct the image.</div>\n",
    "<div style=\"margin:50px\"><img src=\"generated_images/gan.png\" alt=\"Italian Trulli\" width=\"90%\"> </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "The generator and the discriminator will start a challenge in a way that the generator will try to fool the discriminator generating images that it cannot predict wether they are real or fake. This can be done thanks to the loss function.\n",
    "The discriminator loss function will have to parts, the error comming from real images which will try to minimize the difference between the predicted targets and a tensor with all ones (all ones mean that they are all real), and the error comming from the generator (fake images) which will try to minimize the difference between a tensor with all zeros (all zeros mean they are fake) and the images generated with the generator which should be considered fake by the discriminator. \n",
    "The generator loss function has to minimize the difference between the output of the discriminator when it has been fed with fake that and a tensor of all ones (meaning all real). This way the minmax game competition starts!\n",
    "Next we can see two different types of pictures generated by the the GAN built in the code section. The CIFAR10 and a human face dataset are used for such approach. We can see how the images get more and more realistic as the neural network is trained.\n",
    "</div>\n",
    "\n",
    "<div style=\"font-size: 40px; font-weight: bold; font-family:Garamond; text-align:center; margin: 20px; margin-top: 35px\">GENERATED IMAGES</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div><img src=\"images/animation.gif\"> </div>\n",
    "<div><img src=\"images/face.gif\"></div>\n",
    "\n",
    "<!--\n",
    "<div style=\"font-size: 40px; font-weight: bold; font-family:Garamond; text-align:center; margin: 20px; margin-top: 35px; margin-bottom:40px\">REAL IMAGES</div>-->\n",
    "\n",
    "<!--\n",
    "<img src=\"generated_images/real_samples_cifar.png\" style=\"padding: -100px -100px\">    \n",
    "<img src=\"generated_images/real_samples_faces.png\">-->\n",
    "\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Applications </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "GANs and its variations (Pix2Pix, StyleGANs ...) which contain a GAN structure,  have lots of differents applications. They can be used for image editing. Imagine you have taken pictures but the landscape cannot be clearly seen since there was mist. You could use a deep learning achitecture using the GANs structure to obtain neat images. Another example is the famous application named FaceApp which can make you look like an old person. In the next example we can see how a GAN architecture can perform a mapping from one picture (young) to the other picture (old).\n",
    "In the Pix2Pix tutorial we will get more in detail about this GAN-like architecture\n",
    "</div>\n",
    "<img src=\"generated_images/old.gif\" style=\"height:400px; width:600px; margin-top:-10px\">\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "Another application is to generate data. Nowadays, Artificial intelligence has a major drawback, we need lots and lots of data in order to build a robust model that can perform properly. However, in many fields there is a lack of data and a necessesity to obtain more and more datasets. GANs can be used for such taks, creating new datasets that can be combined with other types of supervised learning in order to obtain reliable machine learning models.\n",
    "</div>\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Conclusions </div>\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Code </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data \n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from functools import wraps\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utilities(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def timing(minutes=False):\n",
    "        def mydecorator(func):\n",
    "            @wraps  #Not to lose information of the native function!\n",
    "            def mywrapper(self, *args, **kwargs):\n",
    "                start = time.time()\n",
    "                return_variable = func(self, *args, **kwargs)\n",
    "                end = time.time()\n",
    "                if minutes:\n",
    "                    final_time = (end - start) / float(60)\n",
    "                    print(\"The execution time for function {} is: {} minutes\".format(func.__name__, final_time))\n",
    "                else:\n",
    "                    final_time = (end - start)\n",
    "                    print(\"The execution time for function {} is: {} seconds\".format(func.__name__, final_time))\n",
    "                    \n",
    "                return return_variable\n",
    "            return mywrapper\n",
    "        return mydecorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataPreparation(object):\n",
    "    \n",
    "    def __init__(self, batch_size, image_size, num_epochs):\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._image_size = image_size\n",
    "        self._path_images =\"/Users/rubencr/Desktop/ML_notebooks/ML_models/ML_tutorials_github\"\n",
    "        self._path_save = os.path.join(os.getcwd(), \"generated_images\")\n",
    "        self._num_epochs = num_epochs\n",
    "        self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "    \n",
    "    @property\n",
    "    def num_epochs(self):\n",
    "        return self._num_epochs\n",
    "\n",
    "\n",
    "    def _preprocess(self):\n",
    "\n",
    "        #Image transformations (applying different transformations)\n",
    "        transform = transforms.Compose([transforms.Scale(self._image_size),\n",
    "        transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5)),])\n",
    "\n",
    "        # Loading the dataset\n",
    "        dataset = dset.CIFAR10(root=self._path_images, download = True, transform = transform)\n",
    "        #dataset = dset.ImageFolder(root=self._path_images, transform=transform) image folder for loading other images\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self._batch_size, shuffle=True, num_workers = 2)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "        \n",
    "class DiscriminatorNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__() #Initializing superclass\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            #In pytorch out_channels equals number of filters applied\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0, bias = False),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.model(input)\n",
    "        return output.view(-1) #View method reshapes!\n",
    "\n",
    "    \n",
    "class GeneratorNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=100, out_channels=512, kernel_size=4, stride=1, padding=0, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
    "            nn.Tanh() #The output is squased from -1 to 1 using the parabolic tangent function\n",
    "\n",
    "            )\n",
    "        \n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "    \n",
    "class Trainer(DataPreparation):\n",
    "\n",
    "    def __init__(self, batch_size=64, image_size=64, num_epochs=10):\n",
    "        super(Trainer, self).__init__(batch_size, image_size, num_epochs)\n",
    "        \n",
    "        self.__discriminator = DiscriminatorNet()\n",
    "        self.__generator = GeneratorNet()\n",
    "        self.__criterion = nn.BCELoss()\n",
    "        self.__optimizer_discriminator = optim.Adam(self.__discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.__optimizer_generator = optim.Adam(self.__generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.__dataloader = self._preprocess()\n",
    "        \n",
    "        \n",
    "    #@Utilities.timing(minutes=True)\n",
    "    def train(self):\n",
    "        \n",
    "        for epoch in range(self._num_epochs):\n",
    "            for i, data in enumerate(self.__dataloader,0):\n",
    "\n",
    "                #Now we train the discrimnator which are the real data and the classifier label for\n",
    "                #real data is one, this is why the target is built using a torch.ones which builds a matrix\n",
    "                #in which all the values equals one, and of course the size of the squared-matrix is the input size!\n",
    "\n",
    "                self.__discriminator.zero_grad() #Gradients set to zero!\n",
    "                real, _ = data\n",
    "                inp = Variable(real)\n",
    "                target = Variable(torch.ones(inp.size()[0]))\n",
    "                output = self.__discriminator(inp)\n",
    "                errD_real = self.__criterion(output, target)\n",
    "\n",
    "\n",
    "                # NOw we train the discriminator with the fake data which labels are 0 and thus, we use the torch.zeros function!\n",
    "\n",
    "                noise = Variable(torch.randn(inp.size()[0], 100, 1, 1))\n",
    "                fake = self.__generator(noise)\n",
    "                target = Variable(torch.zeros(inp.size()[0]))\n",
    "                output = self.__discriminator(fake.detach()) #Using descriminator with the fake that comes from the generator\n",
    "                errD_fake = self.__criterion(output, target) #Minimizing the different between all zeros (False) \n",
    "                #and the images produced by the generator that should be all zero as well.\n",
    "\n",
    "                errD = errD_real + errD_fake\n",
    "                errD.backward()\n",
    "                self.__optimizer_discriminator.step()\n",
    "\n",
    "\n",
    "                #NOW THE GENERATOR IS TRAINED!!\n",
    "\n",
    "                self.__generator.zero_grad()\n",
    "                target = Variable(torch.ones(inp.size()[0]))\n",
    "                output = self.__discriminator(fake)\n",
    "                errG = self.__criterion(output, target) #Here we want the discriminator to think that the fake are ones\n",
    "                #since ones mean not fake!\n",
    "                errG.backward()\n",
    "                self.__optimizer_generator.step()\n",
    "\n",
    "                #print(\"loss_g {} loss_d {}\".format(errG.data, errD.data))\n",
    "                print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 50, i, len(self.__dataloader), errD.data, errG.data))\n",
    "                if i % 100 == 0:\n",
    "                    vutils.save_image(real, '%s/real_samples.png' % self._path_save, normalize = True)\n",
    "                    fake = self.__generator(noise)\n",
    "                    vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (self._path_save, epoch), normalize = True)\n",
    "\n",
    "                        \n",
    "                        \n",
    "def main():\n",
    "    \n",
    "    trainer = Trainer()\n",
    "    trainer.train()\n",
    "                        \n",
    "                        \n",
    "main()            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
