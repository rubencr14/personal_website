{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:40px; font-weight:bold; margin:20px; margin-bottom:100px; text-align: justify;text-shadow: 1px 1px 1px #919191,\n",
    "        1px 2px 1px #919191,\n",
    "        1px 3px 1px #919191,\n",
    "        1px 4px 1px #919191,\n",
    "        1px 5px 1px #919191,\n",
    "        1px 6px 1px #919191,\n",
    "        1px 7px 1px #919191,\n",
    "        1px 8px 1px #919191,\n",
    "        1px 9px 1px #919191,\n",
    "        1px 10px 1px #919191,\n",
    "    1px 18px 6px rgba(16,16,16,0.4),\n",
    "    1px 22px 10px rgba(16,16,16,0.2),\n",
    "    1px 25px 35px rgba(16,16,16,0.2),\n",
    "    1px 30px 60px rgba(16,16,16,0.4)\">Long Short Term Memory Recurrent Networks</div>\n",
    "\n",
    "<div style=\"font-style: italic; font-weight: bold; font-size:35px; text-align:center; font-family: Garamond\">by Rubén Cañadas Rodríguez</div>\n",
    "\n",
    "<div style=\"font-size: 30px; margin: 20px; margin-bottom: 40px; margin-left: 0px; line-height: 40pt\">\n",
    "\n",
    "<div style=\"font-size: 30px; font-family: Garamond; font-weight: bold; margin: 30px; margin-left: 0px; margin-bottom: 10px; \">Contents</div>\n",
    "<ol>\n",
    "<li>Introduction</li>\n",
    "<li>Recurrent Neural Networks (RNNs)</li>\n",
    "<li>Long Short Term Memory (LSTM)</li> \n",
    "<li>Text generation</li> \n",
    "<li>Coding</li> \n",
    "</ol>\n",
    "</div>\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Introduction </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "In this tutorial we will be adressing the problem of generating new data (generative model) based on previous data. This generative model is quite different from autoencoders and GANs based methods since the dataset is previous data.\n",
    "This allows to model times series data that cannot be addresed from Vanilla Neural Networks or Convolutional ones. In this case we are going to use a library built upon Tensorflow named Keras which is a high-level library allowing to code sophisticated deep lerning architectures in just a few lines of code. From Tensforflow 2.0 version Keras is included in Tensorflow! In other tutorials we will implement deep learning architecture using Tensorflow 2.0 and explaining its new options.\n",
    "</div>\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Recurrent Neural Networks (RNNs) </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "Recurrent Networks are a special deep learning architecture that allows to work with sequence data. For example we could use time series data to predict the next data values in time. A very visual example is when we have the stock prices for a period of time and we want to predict the future values based on the past. RNNs can be used for such task as we can see in the following figure:\n",
    "<img src=\"images/rnn_prediction.png\" width=\"80%\" style=\"text-align: center; margin: 30px; margin-bottom: 40px\"> \n",
    "How does this type of architecture allow data prediction? To achieve this goal we have to re-design the traditional neural networks. In conventional architectures we have a fixed-sized vector of inputs and fixed vector of outputs. Howevever, in RNNs we can have vectors of sequences allowing us for example to perform sentiment analysis where the input is a sequence of data and the output is a label (e.g. sad, happy, excited..) is known as a many-to-one architecture. Also (as we will see in this tutorial) RNNs can be used for text generation. In this case, the input is a sequence of data and the output is also a sequence, therefore, it is a many-to-many architecture. This allows to generate meaningful text. In the next figure we can see the different types of RNNs depending on the form of input/output data.\n",
    "<img src=\"images/rnn_arc.jpg\" width=\"80%\" style=\"text-align: center; margin: 30px; margin-bottom: 40px; margin-leftt:100px\"> \n",
    "RNNs work pretty well for short sequences. Nevertheless, in practice, where long-term dependencies are need in extensive sequences, conventional RNNs fail due to a lack of \"memory\". This happens due to the backpropagation where gradients get smaller and smaller and thus, the network becomes more complicated to train. For more information with the problems of gradient descent when dealing with long-term dependencies chek this paper by Bengio and its co-workers <a href=\"http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf\" style=\"text-decoration:none\">Bengio, et al. (1994)</a> Different solutions have been proposed concerning some variations of the conventional RNNs: Long Short Term Memory networks (LSTM) and Gated Recurrent Units (GRU).\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Long Short Term Memory (LSTM)\n",
    "</div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "The key idea is that the network can learn what to store in the long-term state, what to throw away and what to read from it. There are two different states: the cell state which is the long-term state $C_{t}$ and the short-term (hidden state) $h_{t}$\n",
    "As the long-term state traverses the network from left to right, you can see that it first goes through a forget gate, dropping some of the memories, and then it adds some new memories via the addition operation, which adds the memories that were selected by an input gate. This allows the neural network which information of the past is relevant and what \"memory\" to preserve. In the following figure, a schematic picture of a LSTM cell is shown. \n",
    "</div>\n",
    "<img src=\"lstm.png\" width=\"80%\" style=\"text-align: center; margin: 30px; margin-bottom: 40px; margin-leftt:100px\"> \n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Text generation </div>\n",
    "One application of LSTM is text generation in which given a sequence of characters from given data, train a model to predict the next character in the sequence. In this tutorial we are going to use a quote dataset, where thousands of quotes said by important people throughout the history are gathered. Then, we will try to build a model that can create quotes with logical meaning. The first step is to load the dataset and preprocess it so that the method can use the data. Strange characters are removed and also white spaces. Then new sentences are created. In each new sentence we add an additional letter. For example, if we have the sentence: I live in New York, this list of sentences will be created: [\"I \", \" l\", li\", \"iv\", \"ve\", \" i\", \"in\", \"n \", \" N\", \"Ne\", \"ew\", \"w \", \" Y\", \"Yo\", \"or\", \"rk\"]. Also a list of the next character for each sentence is created. Then, a three-dimensional tensor (X) is fed where the first dimension is the number of sentences, the second dimension is the number of chars, and the third dimension is the mapping from char-to-index. For the prediction (Y) the tensor is two-dimensional: the first dimension is the number of sentences, and the second dimension the corresponding next character (char-to-indices). This way the recurrent neural network with LSTM cells is fit and trained. Depending on the number of epochs and the batch size we will obtain better or worse results. \n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Coding </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re #regular expressions for treating text\n",
    "import numpy as np\n",
    "from nltk import word_tokenize #Natural Language Processing package\n",
    "from nltk import word_tokenize\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Input, Activation, Dense, Dropout\n",
    "from keras.layers import LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation(object):\n",
    "    \n",
    "    def __init__(self, max_lenght, step):\n",
    "        \n",
    "        self._csv = \"QUOTE.csv\"\n",
    "        self.__df = pd.read_csv(self._csv)\n",
    "        self.__quotes = list(self.__df.quote + \"\\n\") #adding a break at the end of each quote!\n",
    "        self.__chars_to_remove = ['#', '$', '%', '(', ')', '=', ';' ,':',  '*', '+', '£' , '—','’']\n",
    "        self.__cleaned_quotes = []\n",
    "        self._chars = None\n",
    "        self._char_indices = None\n",
    "        self._indices_char = None\n",
    "        self._sentences = []\n",
    "        self._next_chars = []\n",
    "        self._max_length = max_lenght\n",
    "        self._step = step #From where to start the next sentences ej: \"If you live to \" the next will be\n",
    "        #with a step of 6: \" live to be a h\" and the next \"to be a hundred\" so we build sentences using\n",
    "        #step of 6 words between. This is what is known as Bag of N-grams. This parameter can be tweaked. \n",
    "        \n",
    "    def __str__(self):  \n",
    "        return \"{}\".format(self.__quotes)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.__quotes)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.__quotes[index]\n",
    "    \n",
    "    @property\n",
    "    def chars_to_remove(self):\n",
    "        return self.__chars_to_remove #Getter method for obtaining the default chars to remove\n",
    "    \n",
    "    @chars_to_remove.setter\n",
    "    def chars_to_remove(self, list_of_chars):\n",
    "        self.__chars_to_remove = list_of_chars #Setter method for changing the chars to remove from sentences\n",
    "        \n",
    "        \n",
    "    def __remove_unused_chars(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method removes caracthers that we do not want to include in our model\n",
    "        saved in self.__chars_to_remove attribute. Also we remove more than two spaces\n",
    "        in ours sentences. This method appends the results to cleaned_chars variable\n",
    "        \"\"\"\n",
    "        \n",
    "        for quote in self.__quotes:\n",
    "            for char in self.__chars_to_remove:\n",
    "                new_quote = quote.replace(char, ' ')\n",
    "            pattern = re.compile(r'\\s{2,}') # create the pattern: regular expression for replacing more than two white spaces\n",
    "            quote = re.sub(pattern, ' ', quote)\n",
    "            self.__cleaned_quotes.append(quote)\n",
    "                  \n",
    "    def __obtain_char_indices(self):\n",
    "        \n",
    "        self.__remove_unused_chars() #Creatin cleaned_chars variables that was initialized as empty list\n",
    "        text = ' '.join(self.__cleaned_chars)\n",
    "        self._chars = sorted(list(set(text))) #We extract all the characters (not repeated ) that are present in the sentences\n",
    "        self._char_indices = dict((c, i) for i, c in enumerate(chars)) #To each character we assign a number\n",
    "        self._indices_char = dict((i, c) for i, c in enumerate(chars)) #The contrary, to each number we asssign a character\n",
    "        \n",
    "    \n",
    "    def _generate_sentences(self):\n",
    "        \n",
    "        for quote in self.__cleaned_quotes:\n",
    "            for i in range(0, len(quote) - self._max_length, self._step):\n",
    "                sentences.append(quote[i: i + self._max_length]) #sentence of lenght maxlen\n",
    "                next_chars.append(quote[i + self._max_length]) #next char after sentence of max lenght\n",
    "            self._sentences.append(quote[-self._max_length:])\n",
    "            self._next_chars.append(quote[-1])\n",
    "        self._sentences = self._sentences[:100] #Optional to reduce time consumption we limit the number of sentences\n",
    "\n",
    "    \n",
    "    def _vectorization(self):\n",
    "        \n",
    "        if not self._sentences:\n",
    "            raise ValueError(\"_generate_sentences method has to be applied before vectorizing!! Otherwise execute generate_and_vectorize \")\n",
    "        \n",
    "        x = np.zeros((len(self._sentences), self._max_length, len(self._chars)), dtype=np.bool) #Three dimensional tensor: for each sentence and \n",
    "        #each char of the sentence we assign an index corresponding to a particular char\n",
    "        y = np.zeros((len(self._sentences), len(self._chars)), dtype=np.bool) #Two dimensional tensor, for each sentence (of maxlen) we assign a next\n",
    "        #char that the LSTM will have to guess given X. \n",
    "        for i, sentence in enumerate(self._sentences):\n",
    "            for t, char in enumerate(sentence): \n",
    "                x[i, t, self._char_indices[char]] = 1 # Tensor[sentences, chars, indices_char]\n",
    "            y[i, self._char_indices[self._next_chars[i]]] = 1 # Tensor [sentence, next_char_in_sentence]\n",
    "            \n",
    "        return x,y\n",
    "        \n",
    "    def generate_and_vectorize(self):\n",
    "        \n",
    "        self._generate_sentences()\n",
    "        return self._vectorization() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; margin: 20px; margin-left:-300px\">\n",
    "  <tr>\n",
    "    <th>Predictors (X train)</th>\n",
    "    <th>Labels (Y train)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>they</td>\n",
    "    <td>are</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>they are</td>\n",
    "    <td>learning</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>they are learning</td>\n",
    "    <td>artificial</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <td>they are learning artificial</td>\n",
    "    <td>inteligence</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLSTM(DataPreparation):\n",
    "\n",
    "    def __init__(self, epochs=5, batch_size=10000, max_lenght=15, step=1):\n",
    "        super(TrainLSTM, self).__init__(max_lenght, step)\n",
    "        self.__epochs = epochs\n",
    "        self.__batch_size = batch_size\n",
    "        self.__model = Sequential()\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Model parameters: \\n batch size: {}\\n number of epochs: {}\".format(self.__batch_size, self.__epochs)\n",
    "    \n",
    "    @property    \n",
    "    def num_epochs(self):\n",
    "        return self.__epochs\n",
    "    \n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.__batch_size\n",
    "        \n",
    "    def model(self):\n",
    "\n",
    "        self.__model.add(Bidirectional(LSTM(256, return_sequences= True, \n",
    "                                     input_shape=(self._max_length, len(self._chars))), name = 'bidirectional'))\n",
    "        self.__model.add(Dropout(0.1, name = 'dropout_bidirectional_lstm'))\n",
    "        self.__model.add(LSTM(64, input_shape=(self._max_length, len(self._chars)), name = 'lstm'))\n",
    "        self.__model.add(Dropout(0.1,  name = 'drop_out_lstm'))\n",
    "        self.__model.add(Dense(15 * len(self._chars), name = 'first_dense'))\n",
    "        self.__model.add(Dropout(0.1,  name = 'drop_out_first_dense'))\n",
    "        self.__model.add(Dense(5 * len(self._chars), name = 'second_dense'))\n",
    "        self.__model.add(Dropout(0.1,  name = 'drop_out_second_dense'))\n",
    "        self.__model.add(Dense(len(self._chars), name = 'last_dense'))\n",
    "        self.__model.add(Activation('softmax', name = 'activation'))\n",
    "        self.__model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        model.fit([x], y, batch_size=self.__batch_size, epochs=self.__epochs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
