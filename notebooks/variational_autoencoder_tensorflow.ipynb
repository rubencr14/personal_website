{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--<p style=\"background-image: url('ai_logo.gif'; background-size: 4000px 3000px;\">-->\n",
    "\n",
    "<div style=\"font-size: 60px; font-weight:bold; margin:20px; margin-bottom:100px; text-align: justify; text-shadow: 1px 1px 1px #919191,\n",
    "        1px 2px 1px #919191,\n",
    "        1px 3px 1px #919191,\n",
    "        1px 4px 1px #919191,\n",
    "        1px 5px 1px #919191,\n",
    "        1px 6px 1px #919191,\n",
    "        1px 7px 1px #919191,\n",
    "        1px 8px 1px #919191,\n",
    "        1px 9px 1px #919191,\n",
    "        1px 10px 1px #919191,\n",
    "    1px 18px 6px rgba(16,16,16,0.4),\n",
    "    1px 22px 10px rgba(16,16,16,0.2),\n",
    "    1px 25px 35px rgba(16,16,16,0.2),\n",
    "    1px 30px 60px rgba(16,16,16,0.4)\"> VARIATIONAL AUTOENCODERS </div>\n",
    "    <div style=\"font-style: italic; font-weight: bold; font-size:35px; text-align:center; font-family: Garamond\">by Rubén Cañadas Rodríguez</div>\n",
    "\n",
    "<div style=\"font-size: 30px; margin: 20px; margin-bottom: 40px; margin-left: 0px; line-height: 40pt\">\n",
    "\n",
    "<div style=\"font-size: 30px; font-family: Garamond; font-weight: bold; margin: 30px; margin-left: 0px; margin-bottom: 10px; \">Contents</div>\n",
    "<ol>\n",
    "<li>Introduction</li>\n",
    "<li>Autoencoders</li>\n",
    "<li>Variational autoencoders</li> \n",
    "<li>Coding</li> \n",
    "<li>Conclusions</li> \n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Introduction </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "Generative models are becoming more and more popular in the machine learning community. For instance they have been used in drug discovery to obtain new molecules from the learned distribution representing a chemical subspace. Other examples are data denoising, dimensionality reduction or music creation. Every day, new applications of generative models are appearing in a wide range of fields. These make generative models and in specific, autoencoder and its variations, awesome methods to learn and implement.\n",
    "</div>\n",
    "\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Autoencoders </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "Autoencoders are a special type of deep learning architecture which involves two main pieces: the encoder and the decoder. The input data, for example, images, which can be represented as tensors in a certain vector space, are compressed throughout the encoder until reaching a vector space with an special name: the latent space, which in principle, contains the basic information of the input data. This can be seen as an unsupervised machine learning technique in which the representation dimensions of the input data are shrunk. Then, the decoder uses the latent space two recunstruct the input data. The loss function is how different the decoder output is to the encoder input. This why the two neural networks can be trained so as to to obtain a good representation of the input data in the latent space. \n",
    "</div>\n",
    "<img src=\"images/image.png\" width=\"800\" style=\"padding: 15px\">\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 40px; \"> Variational autoencoders </div>\n",
    "\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "Variational autoencoders (VAEs) are a special type of autoencoder in which a continous distribution function of the training data is infered. Knowing this distribution allows us to sample from there and obtain samples having a high probability of being similar to our training data.\n",
    "First, the encoder tries to approximate the function P(z|x), which is the probability distribution of obtaining z given x. It will obtain the hidden representation in a latent vector space, given the input dataset. Actually, the encoder produces a mean coding $\\mu$ and a standard deviation $\\sigma$. Then, the Gaussian distribution $N(\\mu, \\sigma^{2}$) is sampled, and the decoder approximates the $P(x|z)$ meaning that it recreates the data from the samples obtained from the distribution generated by the encoder\n",
    "</div>\n",
    "<img src=\"images/vae.png\" width=\"800\" style=\"padding: 10px; margin-left:20px\">\n",
    " <!-- <div style=\"color: black; text-align: center; margin: 20px\"> equation -->\n",
    "\n",
    "\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Coding </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "In this example we are going to use the MNIST dataset downloaded directly from tensorflow.examples.\n",
    "First, we specify the different variables:\n",
    "<div style=\"margin: 20px; font-family: Garamond;\"> <!--for ordered list properties-->\n",
    "<ol>\n",
    "<li><b>batch_size</b>: number of instances used in the minibatch gradient descent approximation</li>\n",
    "<li><b>X_in</b>: Defining a placeholder for saving the images used in the training examples. The shape is the shape of the tensor to be fed. None means that it will be the size of the training batches</li>\n",
    "<li><b>Y</b>: This placeholder will hold the images reconstructed by the decoder part of the autoencoder</li>\n",
    "<li><b>Y_flat</b>: Y reshaped in order to be used when computing losses</li>\n",
    "<li><b>keep_prob</b>: This is used as a placeholder when using the dropout regularization technique<</li>\n",
    "<li><b>reshaped_dim</b>: dimension of the decoder when sarting the transpose convolutional2D layers after the dense layers\n",
    "architecture</li>\n",
    "<li><b>inputs_decoder</b>: The dimension of the inputs in the decoder phase</li>\n",
    "</ol>\n",
    "</div>\n",
    "The leaky_relu is defined since tensorflow does not have a predefined one (since 1.4.0 release tf.nn.leaky_relu can be used)\n",
    "\n",
    "The encoder if the first part of the architecture. In this phase, the aim is to find a lower dimensional representation of the images until obtaining the latent variables Z. Firstly, the images are reshaped according to the tensor dimensions:\n",
    "[batch_size, height, width, channels]. -1 in batch_size means using the batch size defined when splitting the dataset\n",
    "into batches. RGB for example has 3 channels whereas white and black images have 1 as channels.\n",
    "The encoder has basically convolutional layers since we are treating with images. However, other types of architectures could be employed instead. \n",
    "\n",
    "<img src=\"images/conv.jpeg\" width=\"800\" style=\"margin-left:20px\">\n",
    "\n",
    "The mean and standard deviation necessary for the reparametrization trick will be computed using two dense layers which\n",
    "will be optimized for computing them via the gradients.\n",
    "The decoder can sample from Q(Z|X) distribution in order to obtain the images back to their real form. Nonetheless, since sampling is stochastic we cannot use backpropagation! To solve this we can apply: z = mean + deviation * epsilon (* meaning elementwise or Hadamard multiplication). This way, all the randomness is saved in the epsilon variables and now\n",
    "we can use gradients in order to optimize the theta and phi parameters. This is called the reparametrization trick.\n",
    "Then, the decoder can sample from the learned representation and use deconvolutational layers in order to perform the reverse process: construct the image given the low-dimensional representation (latent space). </div>\n",
    "<!--<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Conclusions </div>-->\n",
    "\n",
    "</div>\n",
    "<div style=\"font-size: 30px; font-weight:bold; margin: 20px; margin-top:30px; margin-bottom: 30px; text-align: justify\"> <b>Importing packages and modules</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from functools import wraps\n",
    "import os, sys, time\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 30px; font-weight:bold; margin: 20px; margin-top:30px; margin-bottom: 30px; text-align: justify\"> <b>Decorators</b> </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "This section include some interesting decorators that will be used in the autoencoder architecture.\n",
    "Basically decorators are methods that modify the functionality of the function they decorate. The tensorboard\n",
    "graph decorator produces a the network graph that can be viewed in a given URL using tensorboad utility.\n",
    "The timing decorator allows to know the execution time of a certain function. The staticmethod decorator defines a method that is not instance dependent. It only depends on the class in which it is found. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decorators(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def tensorboad_graph(path=os.getcwd()):\n",
    "        \n",
    "        \"\"\"\n",
    "        This decorator allows to create the graph in tensorboard.\n",
    "        run this command on the terminal in the directory \"path\": python -m tensorboard.main --logdir=.\n",
    "        and then enter the following URL: http://localhost:6006/\n",
    "        \n",
    "        \"\"\"\n",
    "        def mydecorator(func):\n",
    "            @wraps(func)\n",
    "            def mywrapper(self, *args, **kwargs):\n",
    "                func(self, *args, **kwargs)\n",
    "                with tf.Session() as sess:\n",
    "                    writer = tf.summary.FileWriter(os.path.join(path, \"graphs\"), sess.graph)\n",
    "                    msg = \\\n",
    "                    \"\"\"\n",
    "                    run this command on the terminal in the directory \"path\": \n",
    "                    python -m tensorboard.main --logdir=.\n",
    "                    and then enter the following URL: http://localhost:6006/\n",
    "                    \"\"\"\n",
    "                    print(\"INSTRUCTIONS FOR TENSFORBOARD: \\n {}\".format(msg))\n",
    "            return mywrapper\n",
    "        return mydecorator\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def timing(func):\n",
    "        \n",
    "        \"\"\"\n",
    "        This decorator allows to know how much time is the execution time of the\n",
    "        decoratedd function/method in seconds\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        @wraps(func)\n",
    "        def mywrapper(self, *args, **kwargs):\n",
    "            start = time.time()\n",
    "            return_variable = func(self, *args, **kwargs)\n",
    "            end = time.time()\n",
    "            print(\"Execution time of method {}: {} seconds\".format(func.__name__, \n",
    "                                                                   round(end-start, 3)))\n",
    "            return return_variable\n",
    "        return mywrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 30px; font-weight:bold; margin: 20px; margin-top:30px; margin-bottom: 30px; text-align: justify\">\n",
    "Data preparation and network architecture </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "The PrepareData class is the parent class of the precding classes since it has all the placeholders and values\n",
    "defining the structure of the encoder and decoder. In this example we work with the MNIST dataset that holds 28x28 images, as mentioned before. The placeholders will hold this data when the session is initilized. Now we are only defining the static graph (in Pytorch or Tensorflow 2.0 dynamic graphs can be defined). Most of the variables have been defined above. Inputs decoder means the number of instances that are going to be sampled from the z distribution built in the encoder phase. The reshaped_dim is the shape of the sampled instances since we need a 4-dimensional vector to feed the convolutional layers. Finally the inputs_decoder refer to the number of instances are sampled from the learned distibution and will fed into the decoder layer. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData(object):\n",
    "\n",
    "    def __init__(self, batch_size, latent_variables, dec_in_channels):\n",
    "        \n",
    "        self._mnist =  input_data.read_data_sets(\"MNIST\", one_hot=False)\n",
    "        self._X_in = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28], name=\"X\")\n",
    "        self._Y = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28], name=\"Y\")\n",
    "        self._Y_flat = tf.reshape(self._Y, shape=[-1, 28 * 28])\n",
    "        self._keep_prob = tf.placeholder(dtype=tf.float32, shape=(), \n",
    "                                         name=\"keep_prob\") #Used for dropout layers\n",
    "        self._number_of_latent_variables = latent_variables\n",
    "        self._batch_size = batch_size\n",
    "        self._dec_in_channels = dec_in_channels\n",
    "        self._reshaped_dim = [-1, 7, 7, self._dec_in_channels]\n",
    "        self._inputs_decoder = 49\n",
    "        \n",
    "    def __str__(self):\n",
    "        \n",
    "        return \"variables: \\n latent variables: {} \\n batch size: {}\".format(self._number_of_latent_variables, \n",
    "                                                                             self.batch_size)\n",
    "    \n",
    "class AutoencoderArchitecture(PrepareData):\n",
    "    \n",
    "    def __init__(self, batch_size, latent_variables, dec_in_channels):\n",
    "        super(AutoencoderArchitecture, self).__init__(batch_size, latent_variables, dec_in_channels)\n",
    "        \n",
    "    def leaky_relu(self, x, alpha=0.3):\n",
    "        return tf.maximum(x, tf.multiply(x, alpha))\n",
    "\n",
    "    def _encoder(self):\n",
    "        with tf.variable_scope(\"encoder\", reuse=None):\n",
    "            X_reshaped = tf.reshape(self._X_in, shape=[-1, 28, 28, 1]) #reshape where -1 means the batch_size, \n",
    "                                                                #28 the height, 28 the width and 1 the number of channels!\n",
    "            X = tf.layers.conv2d(X_reshaped, filters=64, kernel_size=4, strides=2,\n",
    "                                 padding=\"same\", activation=self.leaky_relu)\n",
    "            X = tf.nn.dropout(X, self._keep_prob)\n",
    "            X = tf.layers.conv2d(X, filters=64, kernel_size=4, strides=2,\n",
    "                                 padding=\"same\", activation=self.leaky_relu)\n",
    "            X = tf.nn.dropout(X, self._keep_prob)\n",
    "            X = tf.layers.conv2d(X, filters=64, kernel_size=4, strides=1, \n",
    "                                 padding=\"same\", activation=self.leaky_relu)\n",
    "            X = tf.nn.dropout(X, self._keep_prob)\n",
    "            X = tf.contrib.layers.flatten(X) \n",
    "            mean = tf.layers.dense(X, units=self._number_of_latent_variables)\n",
    "            sd = 0.5 * tf.layers.dense(X, units=self._number_of_latent_variables)\n",
    "            epsilon = tf.random_normal([tf.shape(X)[0], self._number_of_latent_variables])\n",
    "            z = mean + tf.multiply(epsilon, tf.exp(sd)) \n",
    "            #tf.multiply is elementwise or Hadamard multiplication!\n",
    "\n",
    "            return z, mean, sd\n",
    "\n",
    "    def _decoder(self, sampled_z):\n",
    "\n",
    "        with tf.variable_scope(\"decoder\", reuse=None):\n",
    "            X = tf.layers.dense(sampled_z, units=self._inputs_decoder, activation=self.leaky_relu)\n",
    "            X = tf.layers.dense(X, units=self._inputs_decoder, activation=self.leaky_relu)\n",
    "            X = tf.reshape(X, self._reshaped_dim)\n",
    "            X = tf.layers.conv2d_transpose(X, filters=64, kernel_size=4, strides=2, \n",
    "                                 padding=\"same\", activation=tf.nn.relu)\n",
    "            X = tf.nn.dropout(X, self._keep_prob)\n",
    "            X = tf.layers.conv2d_transpose(X, filters=64, kernel_size=4, strides=1, \n",
    "                                 padding=\"same\", activation=tf.nn.relu)\n",
    "            X = tf.nn.dropout(X, self._keep_prob)\n",
    "            X = tf.layers.conv2d_transpose(X, filters=64, kernel_size=4, strides=1, \n",
    "                                 padding=\"same\", activation=tf.nn.relu)\n",
    "            X_flatten = tf.contrib.layers.flatten(X)\n",
    "            X_dense = tf.layers.dense(X_flatten, units=28*28, activation=tf.nn.sigmoid)\n",
    "            img = tf.reshape(X_dense, shape=[-1, 28, 28])\n",
    "\n",
    "            return img\n",
    "\n",
    "\n",
    "class AutoencoderTrainer(AutoencoderArchitecture):\n",
    "    \n",
    "    def __init__(self, batch_size=64, latent_variables=20, dec_in_channels=1, iterations=100):\n",
    "        super(AutoencoderTrainer, self).__init__(batch_size, latent_variables, dec_in_channels)\n",
    "\n",
    "        self._sampled, self._mean, self._standard = self._encoder()\n",
    "        self.__iterations = iterations\n",
    "\n",
    "    @property\n",
    "    def iterations(self):\n",
    "        return self.__iterations\n",
    "    \n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "    \n",
    "    @property\n",
    "    def latent_variables(self):\n",
    "        return self._number_of_latent_variables\n",
    "\n",
    "    @Decorators.timing\n",
    "    def train(self):\n",
    "\n",
    "        dec = self._decoder(self._sampled)\n",
    "        unreshaped = tf.reshape(dec, [-1, 28 * 28])\n",
    "        img_loss = tf.reduce_sum(tf.squared_difference(unreshaped, self._Y_flat), 1) \n",
    "        #Log-likelihood \n",
    "        latent_loss = -0.5 * tf.reduce_sum(1.0 + 2.0 * self._standard - tf.square(self._mean) \\\n",
    "                                           - tf.exp(2.0 * self._standard), 1) \n",
    "                                            #Kullback-Leibler divergence\n",
    "        \n",
    "        loss = tf.reduce_mean(img_loss + latent_loss)\n",
    "        optimizer = tf.train.AdamOptimizer(0.0005).minimize(loss)\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer()) #Initializing variables\n",
    "        for i in range(self.__iterations):\n",
    "            batch = [np.reshape(b, [28, 28]) for b in \\\n",
    "                     self._mnist.train.next_batch(batch_size=self._batch_size)[0]]\n",
    "            labels = self._mnist.train.next_batch(batch_size=self._batch_size)[1]\n",
    "            sess.run(optimizer, feed_dict={self._X_in: batch, \n",
    "                                           self._Y: batch, self._keep_prob: 0.8})\n",
    "        ModelAnalysis(sess, self._number_of_latent_variables, dec, \n",
    "                      self._sampled, self._keep_prob)._data_generator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 30px; font-weight:bold; margin: 20px; margin-top:30px; margin-bottom: 30px; text-align: justify\">\n",
    "Data generation </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "    Once the internal representation has been learned, instances can be sampled out of it and new data can be generated following the learned distribution. The following class is the responsible of doing such task. Then, this class is called from the AutoencoderTrainer class after the training has been completed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAnalysis(object):\n",
    "    \n",
    "    def __init__(self, sess, num_latent, decoder, sampled_distribution, keep_prob):\n",
    "        \n",
    "        self.__sess = sess\n",
    "        self.__num_of_latent_variables = num_latent\n",
    "        self.__decoder = decoder\n",
    "        self.__sampled_distribution = sampled_distribution\n",
    "        self.__keep_prob = keep_prob\n",
    "        self.__number_of_images = 5\n",
    "        \n",
    "        \n",
    "    def _data_generator(self):\n",
    "        \n",
    "        randoms = [np.random.normal(0, 1, \n",
    "                                    self.__num_of_latent_variables) for _ in range(self.__number_of_images)]\n",
    "        imgs = self.__sess.run(self.__decoder, \n",
    "                               feed_dict={self.__sampled_distribution: randoms,\n",
    "                                          self.__keep_prob: 1.0})\n",
    "        \n",
    "        imgs = [np.reshape(imgs[i], [28, 28]) for i in range(len(imgs))]\n",
    "        for img in imgs:\n",
    "            plt.figure(figsize=(1,1))\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "            plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 30px; font-weight:bold; margin: 20px; margin-top:30px; margin-bottom: 30px; text-align: justify\">\n",
    "Main </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:20px; margin: 20px; margin-left: 0px; line-height: 24pt\">\n",
    "    This function create the objects and launches the job.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tensorflow 1.14.0 for generating the variational autoencoder\n",
      "Extracting MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "            PARAMETERS \n",
      "\n",
      "\n",
      "    Number of iterations: 400 \n",
      "\n",
      "    Batch size: 64 \n",
      " \n",
      "    Number of latent variables: 49 \n",
      "\n",
      "    \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFkAAABYCAYAAACeV1sKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABR5JREFUeJztnNdO61wQRleoCS30IooAISTuef8X4Am4AQlCJyEQWiDlv0CfnWywODkJg/OfWTfG245jRsvj2WNDptls4vwsA799Av8CHmQDPMgGeJAN8CAb4EE2wINsgAfZgCHLL8tkMv/bmU+z2cwkbXOTDfAgG+BBNsA0J/80mUxiWvyEZWPMTTagL02WsQMDA21L0Ww2ozHtW6/XARgZGWn7jNbv7+8BaDQa0TF6hZtsQF+YLOsGBwfbxmWpxsfGxgDIZrPMzMwAUKvV2rZp/PX1FYBCoQDEBr+/v3+57MZsN9mAVJosQ4eGPk5PpoY5OJfLATA+Pg7A/Pw8ANvb2+zt7QGwuLgIwNvbGwCPj48AXF9fA7CysgLA8fFx23eXy2UALi8vAXh+fgZi4zvBTTYgVSaHVYNMDvPh1NRU23J5eRmAra0tAPb391lfXwdi21VFyEzZL7NnZ2fbziGbzQJxTpfZWnaCm2xAqkwWw8PDAJ9qXY0rR09MTACwtLQEfORi+LD27u4u+hniKkH2n56etq2Pjo4CcH5+DsTVh1Au1jl1kpvdZANSZbIsCXNwODuTdTJ5cnISiGvhWq0WmascOjc3B8DT0xMALy8vQDwTDL9TM0CRdH/4o9+r4084HZMqk4Xu6Mp7skgmh+uqDDQOsaHVahWAYrEIwM3NDRDXvTK2Uqm0rWu7ziXsj3SCm2xAqkxWvktaatYmq1QZ7O7uAnGuLpVKUf07PT0NxEYrV6tSeXh4AOD29haIjVfO1tXUTXfOTTagL0xWnRzmx52dHSDOybK2Xq9HVmtMx1K1odyrejhcht0378KlnFSZnERotMxV/0EzPuXofD4fVRfaVwZrRqerQnWzcnZYN/fiCYmbbEBfmCxUB4dPMbSuztnGxkZUPYRmah8dS0armuilwdF59+xIPSDpF9O4bnjhtFrBVjppNptRi1NBVKmmZr3Ww1T0E3i6MCBVJicRPjDVzUpTYW3XstFoRCWazNUDU6UPTWyUVrT8CdxkA1JtcmiwlsrJ2q4GvFqdmUwmupGdnZ0B8bRZuVgoZ4f3g9b83i1usgGpMjm8w8tc5UsZrCpDeVUPR1WWlctlSqUSEOdk7aNps3KzWpoaT6o2fFqdclJhcvgqgNbVlFc9rGnz6uoq8HlyooZ8sViMzJTRysX6jFqhYSszrFREuF8nuMkGpNLk8JUA5WY9DF1YWADiykCzu9YHsaoudDVoqXE1hGSmvuO7B6ZuckpJpclhw0f1bz6fB2LLNjc3gdhsXQGVSiXaR30NHUvVhrYrd4d1cevssXX8b3CTDUiFyd91wvTyivKqcrCqDL0yK1sLhUJ0VcjUk5MTAC4uLoB4BqgqQ8brc6qju6kqhJtsQKpMli0yNql+VgWgR0tra2tAPHvL5XJR7lXv4uDgAIhnfnohMawywl5G0mtcneAmG5Aqk5UXw1o1fAiqmlY5WZ/T9sPDQ66urgA4OjoCYqNlsHrS4Z8pJL2W0A1usgEZyz9//dN/xaDcqypCZrc+KIW4PtYLLLK1Wq1GMzs9IdHVkPTyivjbePi/YvhlUmlySNjDUF9Z567x1spAVUFSdRD+OXC3uMm/TF+Y/MVxgM/181cVQa9y7ne4yb9MX5qcRtzkX8bU5H8VN9kAD7IBHmQDPMgGeJAN8CAb4EE2wINsgAfZAA+yAR5kAzzIBniQDfAgG+BBNsCDbIAH2QAPsgEeZAM8yAZ4kA3wIBvgQTbgP27f5oBCoD+QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of method train: 158.328 seconds\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    print(\"Using tensorflow {} for generating the variational autoencoder\".format(tf.__version__))\n",
    "    tf.reset_default_graph()\n",
    "    trainer = AutoencoderTrainer(batch_size=64, latent_variables=49, iterations=400)\n",
    "    parameters = \"\"\"\n",
    "            PARAMETERS \\n\\n\n",
    "    Number of iterations: {} \\n\n",
    "    Batch size: {} \\n \n",
    "    Number of latent variables: {} \\n\n",
    "    \"\"\".format(trainer.iterations, trainer.batch_size, trainer.latent_variables)\n",
    "    print(parameters)\n",
    "    trainer.train()\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
