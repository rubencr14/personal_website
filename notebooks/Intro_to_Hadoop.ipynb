{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 60px; font-weight:bold; margin:20px; margin-bottom:100px; text-align: justify; text-shadow: 1px 1px 1px #919191,\n",
    "        1px 2px 1px #919191,\n",
    "        1px 3px 1px #919191,\n",
    "        1px 4px 1px #919191,\n",
    "        1px 5px 1px #919191,\n",
    "        1px 6px 1px #919191,\n",
    "        1px 7px 1px #919191,\n",
    "        1px 8px 1px #919191,\n",
    "        1px 9px 1px #919191,\n",
    "        1px 10px 1px #919191,\n",
    "    1px 18px 6px rgba(16,16,16,0.4),\n",
    "    1px 22px 10px rgba(16,16,16,0.2),\n",
    "    1px 25px 35px rgba(16,16,16,0.2),\n",
    "    1px 30px 60px rgba(16,16,16,0.4)\"> INTRODUCTION TO HADOOP</div>\n",
    "    <div style=\"font-style: italic; font-weight: bold; font-size:35px; text-align:center; font-family: Garamond\">by Rubén Cañadas Rodríguez</div>\n",
    "\n",
    "<div style=\"font-size: 30px; margin: 20px; margin-bottom: 40px; margin-left: 0px; line-height: 40pt\">\n",
    "<div style=\"font-size: 30px; font-family: Garamond; font-weight: bold; margin: 30px; margin-left: 0px; margin-bottom: 10px; \">Contents</div>\n",
    "<ol>\n",
    "<li><a href=\"#intro\" style=\"text-decoration:none; color:black\">Introduction</a></li>\n",
    "<li><a href=\"#hdfs\" style=\"text-decoration:none; color:black\">Hadoop distributed filesystem</a></li>\n",
    "<li><a href=\"#map\" style=\"text-decoration:none; color:black\">MapReduce</a></li> \n",
    "<li><a href=\"#eco\" style=\"text-decoration:none; color:black\">Hadoop ecosystem</a></li> \n",
    "<li><a href=\"#code\" style=\"text-decoration:none; color:black\">Code</a></li> \n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<div id=\"intro\" style=\"font-size: 3vw; font-weight: bold; margin-bottom:4vh\"> Introduction </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:1.6vw;  margin:0.5vh; margin-right:2vw; line-height: 1.5\">\n",
    "The era of the data and information has arriven, lots of information coming from a huge variaty of sources need to be processed in a considerably short amount of time and analyzed in order to extract useful information. Therefore, it becomes infeasible to use a single machine to process it. Instead, we need lots of machines working together for extracting important information of such enormous datasets. Basically, all the big data technologies allow to carry out algorithms resiliently (fault tolerant) and in a distributed manner.<br>\n",
    "In this tutorial we will learn the basics of Hadoop and its filesystem HDFS and how different frameworks are built onto this framework. Also, some coding will be proposed to give a slight idea of how these technologies would be use in a real life situation.\n",
    "    \n",
    "</div>\n",
    "<div id=\"hdfs\" style=\"font-size: 3vw; font-weight: bold;  margin-top: 6vh;margin-bottom:4vh\"> \n",
    "Hadoop Distributed FileSystem (HDFS) </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:1.6vw;  margin:0.5vh;margin-right:2vw; line-height: 1.5\">\n",
    "HDFS is the distributed filesystem of Hadoop. It has the capacity to store files in clusters containing several nodes which will be read in a sequential manner allowing a horizontal scaling. HDFS gives redundancy since several \"replicas\" of the files are saved in different nodes. This strategy makes HDFS fault tolerant (resilent).<br>\n",
    "The HDFS architecture consists of two types of machines: the Namenode and the Datanode. The Namenode acts as the master node, saving all the metadata (data which specifies the location of the blocks and how they have been split) which allows to reconstruct the files from its blocks. The Datanodes are considered the slaves. They basically store the blocks of each file and also hand the files to the Namenode or to the client server. <br> \n",
    "Using these architecture the files are saved since they are replicated. This way if unfortunatly some node goes down the file can be reconstructed using the copies in other nodes. However, we could think that the Namenode is indeed a single point of failure since we only have one. Nevertheless, there are strategies to avoid this. For instance, NFS storage can be used to save the state of the node. Also, another Namenode machine can be ready to run in case the main Namenode goes down.  All this is controlled by the resource manager named YARN (Yet Another Resource Manager) which basically is the responsible for scheduling and delivering the jobs to the nodes along the cluster. In the following figure we can see the architecture of Hadoop:\n",
    "</div>\n",
    "<img src=\"hdfs.png\" style=\"margin: 6vh\"/>\n",
    "<div id=\"map\" style=\"font-size: 3vw; font-weight: bold;  margin-top: 6vh;margin-bottom:4vh\"> MapReduce </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:1.6vw;  margin:0.5vh;margin-right:2vw; line-height: 1.5\">\n",
    "Basically, MapReduce is a programming paradigm to process distributed data. It consists of two main operations: map which basically applies a transformation to the data and reduce which aggregates the data. We have to be careful with non-associative reduce operations since the reduce computation goes from left to right.<br>\n",
    "Imagine we have a dataset with the ratings of several movies and we need to compute the average rating for each movie. Each movie has been rated for several users and it is distributed using HDFS in a cluster of nodes. How do we compute the average ratings using the MapReduce paradigm? <br>\n",
    "The first step is the mapper, in which we can create key-value pairs: \"movie name\", (rating_value, 1) for instance\n",
    "(\"Toy Story\", (4, 1)), then the results can be sorted according to the key. Finally, using a reducer we can aggregate the results in order to compute the average using the tuple, where the first number is the sum of the total ratings and the second number is the total people rating that movie. This would be an example of a problem solved using the MapReduce paradigm. \n",
    "    \n",
    "    \n",
    "</div>\n",
    "<div id=\"eco\" style=\"font-size: 3vw; font-weight: bold;  margin-top: 6vh;margin-bottom:4vh\"> \n",
    "Hadoop ecosystem</div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:1.6vw;  margin:0.5vh;margin-right:2vw; line-height: 1.5\">\n",
    "This MapReduce example is quite trivial. However, translating every task into the MapReduce paradigm can become really tough sometimes. For this reason, there are many frameworks that are built on top MapReduce which at the same time is built on top of Hadoop. In this section we will give some examples and explain what they are for:\n",
    "    <ul style=\"margin-top:4vh\">\n",
    "        <li><strong>Spark</strong> is a unified analytics engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing.</li> \n",
    "        <li><strong>Hive</strong> software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL-like language.</li>\n",
    "        <li><strong>Sqoop</strong> is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.</li>\n",
    "        <li><strong>Pig</strong> is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs.</li>\n",
    "        <li><strong>Mahout</strong> is a distributed linear algebra framework and mathematically expressive Scala DSL designed to let mathematicians, statisticians and data scientists quickly implement their own algorithms.</li>\n",
    "        <li><strong>Kafka</strong> is an open-source stream-processing software platform developed by LinkedIn and donated to the Apache Software Foundation, written in Scala and Java. </li>\n",
    "        <li><strong>Flume</strong> is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data.</li>\n",
    "        <li><strong>HBase</strong> is an open-source non-relational distributed database modeled after Google's Bigtable and written in Java. </li>\n",
    "        <li><strong>Cassandra</strong> is a free and open-source, distributed, wide column store, NoSQL database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure. </li>\n",
    "        <li><strong>Oozie</strong> is a server-based workflow scheduling system to manage Hadoop jobs. </li>\n",
    "        <li><strong>ZooKeeper</strong> is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. </li>\n",
    "        <li><strong>Storm</strong> is a free and open source distributed realtime computation system. Apache Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<div id=\"code\" style=\"font-size: 3vw; font-weight: bold;  margin-top: 6vh;margin-bottom:4vh\"> Code </div>\n",
    "<div style=\"text-align:justify; font-family: Garamond; font-size:1.6vw;  margin:0.5vh;margin-right:2vw; line-height: 1.5\">\n",
    "The aim of this tutorial was to introduce Hadoop and the Hadoop Distributed FileSystem and see the frameworks that are built onto Hadoop to take advantage of the distributed computing when dealing with tons of data. Therefore, just a little code is included to show how we could use MapReduce in a cluster with Python using the mrjob library. However, in practice, pure MapReduce jobs are not coded anymore. Instead, the different frameworks showed are used and internally they translate the instructions into MapReduce so we do not have to worry about writing our tasks using mappers and reducers.<br>\n",
    "In the next example, we will show how we can use MapReduce to compute the avarage ratings of a movie. After this is coded we have to execute \"python mapreduce.py u.data\" which is the file that contains the information. \n",
    "    <a href=\"https://grouplens.org/datasets/movielens/\">Download the movielens dataset</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MapReduceExample(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                reducer=self.reducer)]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        (userID, movieID, rating, timestamp) = line.split(\"\\t\")\n",
    "        yield rating, (int(rating), 1)\n",
    "            \n",
    "    def reducer(self, key, values):\n",
    "        all_rates, all_counts = 0,0\n",
    "        for rate, c in values:\n",
    "            all_rates += rate\n",
    "            all_counts += c\n",
    "        yield key, round(all_rates / float(all_counts), 3)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    MapReduceExample.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
